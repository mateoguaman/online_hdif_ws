waiting 2s for topics...
Iteration 1/1000000
Iteration 2/1000000
Iteration 3/1000000
Iteration 4/1000000
Iteration 5/1000000
Iteration 6/1000000
Iteration 7/1000000
Iteration 8/1000000
Iteration 9/1000000
Iteration 10/1000000
Training loss: 0.006360442816723868
Training loss: 0.0029336796340295413
Training loss: 0.0032944604589688967
Training loss: 0.0026003551736433605
Training loss: 0.005269556852986423
Training loss: 0.004317169407524277
Training loss: 0.004338432953999087
Training loss: 0.0032412386542322435
Training loss: 0.004079026753624102
Training loss: 0.003386438827931619
Iteration 11/1000000
Iteration 12/1000000
Iteration 13/1000000
Iteration 14/1000000
Iteration 15/1000000
Iteration 16/1000000
Iteration 17/1000000
Iteration 18/1000000
Iteration 19/1000000
Iteration 20/1000000
Training loss: 0.1299539034974603
Training loss: 0.11057865273555194
Training loss: 0.06773863557395778
Training loss: 0.10041920252511057
Training loss: 0.06958240526975588
Training loss: 0.0405635154359459
Training loss: 0.022872443587048246
Training loss: 0.026704170011563774
Training loss: 0.026396600451402796
Training loss: 0.011320598736869282
Iteration 21/1000000
Iteration 22/1000000
Iteration 23/1000000
Iteration 24/1000000
Iteration 25/1000000
Iteration 26/1000000
Iteration 27/1000000
Iteration 28/1000000
Iteration 29/1000000
Iteration 30/1000000
Training loss: 0.09812236807432237
Training loss: 0.11649404291568376
Training loss: 0.06229852308915619
Training loss: 0.01361883637055715
Training loss: 0.04295489781331267
Training loss: 0.030765524933643037
Training loss: 0.028856185018430153
Training loss: 0.023613910851698307
Training loss: 0.023270148088935953
Training loss: 0.014261836446028683
Iteration 31/1000000
Iteration 32/1000000
Iteration 33/1000000
Iteration 34/1000000
Iteration 35/1000000
Iteration 36/1000000
Iteration 37/1000000
Iteration 38/1000000
Iteration 39/1000000
Iteration 40/1000000
Training loss: 0.02818508839481492
Training loss: 0.017896800707923488
Training loss: 0.01589596921237023
Training loss: 0.01162629754699782
Training loss: 0.006798018817375055
Training loss: 0.010173660337115475
Training loss: 0.006130820287688129
Training loss: 0.007710932160247314
Training loss: 0.007997504577887263
Training loss: 0.0065810718791531764
Iteration 41/1000000
Iteration 42/1000000
Iteration 43/1000000
Iteration 44/1000000
Iteration 45/1000000
Iteration 46/1000000
Iteration 47/1000000
Iteration 48/1000000
Iteration 49/1000000
Iteration 50/1000000
Training loss: 0.018257575481101743
Training loss: 0.01190965350463775
Training loss: 0.00904298963493523
Training loss: 0.01291529751574071
Training loss: 0.0029316968530409106
Training loss: 0.008934271216316899
Training loss: 0.005821597016371766
Training loss: 0.011205140714439385
Training loss: 0.007188705449938496
Training loss: 0.004872411961680827
Iteration 51/1000000
Iteration 52/1000000
Iteration 53/1000000
Iteration 54/1000000
Iteration 55/1000000
Iteration 56/1000000
Iteration 57/1000000
Iteration 58/1000000
Iteration 59/1000000
Iteration 60/1000000
Training loss: 0.0023498216011546224
Training loss: 0.002945133757300635
Training loss: 0.006364242788520141
Training loss: 0.00851595689942975
Training loss: 0.002633037028211998
Training loss: 0.007556673137372118
Training loss: 0.004295690701263707
Training loss: 0.00508015192783857
Training loss: 0.0035682073269975648
Training loss: 0.008153759874825958
Iteration 61/1000000
Iteration 62/1000000
Iteration 63/1000000
Iteration 64/1000000
Iteration 65/1000000
Iteration 66/1000000
Iteration 67/1000000
Iteration 68/1000000
Iteration 69/1000000
Iteration 70/1000000
Training loss: 0.006460832228402524
Training loss: 0.006594384709432918
Training loss: 0.0026099623846320493
Training loss: 0.005967384511961124
Training loss: 0.0045503306468760475
Training loss: 0.004688740093338954
Training loss: 0.006096728916436992
Training loss: 0.0021457416325962346
Training loss: 0.004609642192303823
Training loss: 0.006477250698572523
Iteration 71/1000000
Iteration 72/1000000
Iteration 73/1000000
Iteration 74/1000000
Iteration 75/1000000
Iteration 76/1000000
Iteration 77/1000000
Iteration 78/1000000
Iteration 79/1000000
Iteration 80/1000000
Training loss: 0.01729001885549163
Training loss: 0.04261795786227342
Training loss: 0.018942477798815895
Training loss: 0.00615994632195303
Training loss: 0.00875769044395592
Training loss: 0.011733529299513643
Training loss: 0.02411653464350799
Training loss: 0.004405953859037346
Training loss: 0.01277610006425462
Training loss: 0.016842528646915076
Iteration 81/1000000
Iteration 82/1000000
Iteration 83/1000000
Iteration 84/1000000
Iteration 85/1000000
Iteration 86/1000000
Iteration 87/1000000
Iteration 88/1000000
Iteration 89/1000000
Iteration 90/1000000
Training loss: 0.020578894074823904
Training loss: 0.013230199499996613
Training loss: 0.016433907010478175
Training loss: 0.007273158790766288
Training loss: 0.00973564144604656
Training loss: 0.01804615418105153
Training loss: 0.011524797877022947
Training loss: 0.005872541469491383
Training loss: 0.012307246445238201
Training loss: 0.006607273689779275
Iteration 91/1000000
Iteration 92/1000000
Iteration 93/1000000
Iteration 94/1000000
Iteration 95/1000000
Iteration 96/1000000
Iteration 97/1000000
Iteration 98/1000000
Iteration 99/1000000
Iteration 100/1000000
Training loss: 0.023370548477562605
Training loss: 0.03064926955901609
Training loss: 0.006143651756483295
Training loss: 0.005459859079080425
Training loss: 0.009511272353302583
Training loss: 0.006290207435874199
Training loss: 0.016967520030821935
Training loss: 0.004491122948775567
Training loss: 0.005923101839386307
Training loss: 0.004522149372090374
Iteration 101/1000000
Iteration 102/1000000
Iteration 103/1000000
Iteration 104/1000000
Iteration 105/1000000
Iteration 106/1000000
Iteration 107/1000000
Iteration 108/1000000
Iteration 109/1000000
Iteration 110/1000000
Training loss: 0.00908418396002799
Training loss: 0.006259767479891537
Training loss: 0.004837129306450039
Training loss: 0.010924045911130598
Training loss: 0.005767042072610914
Training loss: 0.005789326487240898
Training loss: 0.0029294288862106335
Training loss: 0.0033045533091194297
Training loss: 0.004025040089052658
Training loss: 0.0025749992403152375
Iteration 111/1000000
Iteration 112/1000000
Iteration 113/1000000
Iteration 114/1000000
Iteration 115/1000000
Iteration 116/1000000
Iteration 117/1000000
Iteration 118/1000000
Iteration 119/1000000
Iteration 120/1000000
Training loss: 0.007953511501441954
Training loss: 0.008810228483314909
Training loss: 0.0035253316243564234
Training loss: 0.006828638322182215
Training loss: 0.009350288098379586
Training loss: 0.006419671859159936
Training loss: 0.007950891145219652
Training loss: 0.003058984031389752
Training loss: 0.004344770450031919
Training loss: 0.005841667418732103
Iteration 121/1000000
Iteration 122/1000000
Iteration 123/1000000
Iteration 124/1000000
Iteration 125/1000000
Iteration 126/1000000
Iteration 127/1000000
Iteration 128/1000000
Iteration 129/1000000
Iteration 130/1000000
Training loss: 0.005321759715191497
Training loss: 0.007893269495530703
Training loss: 0.0033358626209996608
Training loss: 0.005474491417533532
Training loss: 0.0031041349694962096
Training loss: 0.003302183381838104
Training loss: 0.002353764771360395
Training loss: 0.006884632282163669
Training loss: 0.0024545467699287156
Training loss: 0.003196133855869013
Iteration 131/1000000
Iteration 132/1000000
Iteration 133/1000000
Iteration 134/1000000
Iteration 135/1000000
Iteration 136/1000000
Iteration 137/1000000
Iteration 138/1000000
Iteration 139/1000000
Iteration 140/1000000
Training loss: 0.0075416074705746405
Training loss: 0.002408753284931473
Training loss: 0.0034428327183040016
Training loss: 0.007370221729908215
Training loss: 0.006733307751879793
Training loss: 0.0068270339494054914
Training loss: 0.004737855008314905
Training loss: 0.006088381095432091
Training loss: 0.004184417638079118
Training loss: 0.005723205875902089
Iteration 141/1000000
Iteration 142/1000000
Iteration 143/1000000
Iteration 144/1000000
Iteration 145/1000000
Iteration 146/1000000
Iteration 147/1000000
Iteration 148/1000000
Iteration 149/1000000
Iteration 150/1000000
Training loss: 0.004794778914665437
Training loss: 0.013554112530922391
Training loss: 0.00629009079298146
Training loss: 0.0032677749118698108
Training loss: 0.0026059676976610846
Training loss: 0.0028523861163457743
Training loss: 0.005745001396804289
Training loss: 0.0034502626714241437
Training loss: 0.0038279653109776373
Training loss: 0.005351367115594523
Iteration 151/1000000
Iteration 152/1000000
Iteration 153/1000000
Iteration 154/1000000
Iteration 155/1000000
Iteration 156/1000000
Iteration 157/1000000
Iteration 158/1000000
Iteration 159/1000000
Iteration 160/1000000
Training loss: 0.007346832120747779
Training loss: 0.01168231651429786
Training loss: 0.004777545577149364
Training loss: 0.004434604476650914
Training loss: 0.008206960553921425
Training loss: 0.00849265218897757
Training loss: 0.003440925656737843
Training loss: 0.008459039070761815
Training loss: 0.004875752296634318
Training loss: 0.003652916940433267
Iteration 161/1000000
Iteration 162/1000000
Iteration 163/1000000
Iteration 164/1000000
Iteration 165/1000000
Iteration 166/1000000
Iteration 167/1000000
Iteration 168/1000000
Iteration 169/1000000
Iteration 170/1000000
Training loss: 0.028105547669662112
Training loss: 0.043801675263493
Training loss: 0.016536121219253498
Training loss: 0.03426727310690769
Training loss: 0.012498666508874063
Training loss: 0.023512112021311652
Training loss: 0.0219411501068091
Training loss: 0.008414653715046775
Training loss: 0.010296268725339031
Training loss: 0.008901386409921273
Iteration 171/1000000
Iteration 172/1000000
Iteration 173/1000000
Iteration 174/1000000
Iteration 175/1000000
Iteration 176/1000000
Iteration 177/1000000
Iteration 178/1000000
Iteration 179/1000000
Iteration 180/1000000
Training loss: 0.01184631654680274
Training loss: 0.014174940181280873
Training loss: 0.016867763013124166
Training loss: 0.019725962756704067
Training loss: 0.00836610523792927
Training loss: 0.006209536039410205
Training loss: 0.019655392262732368
Training loss: 0.020264209439268162
Training loss: 0.004523726426527808
Training loss: 0.006234695949649423
Iteration 181/1000000
Iteration 182/1000000
Iteration 183/1000000
Iteration 184/1000000
Iteration 185/1000000
Iteration 186/1000000
Iteration 187/1000000
Iteration 188/1000000
Iteration 189/1000000
Iteration 190/1000000
Training loss: 0.012952613754578787
Training loss: 0.004415909022436059
Training loss: 0.017517760479289303
Training loss: 0.0029823632649503534
Training loss: 0.009802319807554537
Training loss: 0.016895855314748354
Training loss: 0.006448724542392169
Training loss: 0.0056959463329861945
Training loss: 0.005439343032131035
Training loss: 0.00941273473129959
Iteration 191/1000000
Iteration 192/1000000
Iteration 193/1000000
Iteration 194/1000000
Iteration 195/1000000
Iteration 196/1000000
Iteration 197/1000000
Iteration 198/1000000
Iteration 199/1000000
Iteration 200/1000000
Training loss: 0.009722314583694056
Training loss: 0.0033666205048097286
Training loss: 0.009919099681888652
Training loss: 0.008246851565785375
Training loss: 0.012673970754917985
Training loss: 0.007914939759029472
Training loss: 0.005847259821100792
Training loss: 0.012767020667896901
Training loss: 0.01075913192830816
Training loss: 0.010897006319154407
Iteration 201/1000000
Iteration 202/1000000
Iteration 203/1000000
Iteration 204/1000000
Iteration 205/1000000
Iteration 206/1000000
Iteration 207/1000000
Iteration 208/1000000
Iteration 209/1000000
Iteration 210/1000000
Training loss: 0.007910720678648037
Training loss: 0.020638588609039256
Training loss: 0.008790092331997353
Training loss: 0.005455460939495795
Training loss: 0.010026261335666689
Training loss: 0.008883108252624242
Training loss: 0.005286352721632066
Training loss: 0.00573887802371156
Training loss: 0.005480761806112732
Training loss: 0.008379431491961076
Iteration 211/1000000
Iteration 212/1000000
Iteration 213/1000000
Iteration 214/1000000
Iteration 215/1000000
Iteration 216/1000000
Iteration 217/1000000
Iteration 218/1000000
Iteration 219/1000000
Iteration 220/1000000
Training loss: 0.008850450249176404
Training loss: 0.0077243329143388256
Training loss: 0.005118317384007004
Training loss: 0.01036050338541858
Training loss: 0.004146353064659338
Training loss: 0.006285863884063697
Training loss: 0.0035339291475586813
Training loss: 0.00699611516422264
Training loss: 0.003452720515465042
Training loss: 0.0061982100705319934
Iteration 221/1000000
Iteration 222/1000000
Iteration 223/1000000
Iteration 224/1000000
Iteration 225/1000000
Iteration 226/1000000
Iteration 227/1000000
Iteration 228/1000000
Iteration 229/1000000
Iteration 230/1000000
Training loss: 0.005456224978056658
Training loss: 0.006957508171740967
Training loss: 0.006916255993499018
Training loss: 0.015462910801513394
Training loss: 0.007353285214456889
Training loss: 0.005961554438592335
Training loss: 0.007154561094393059
Training loss: 0.004202265958548378
Training loss: 0.012825595917495382
Training loss: 0.0036701126077564364
Iteration 231/1000000
Iteration 232/1000000
Iteration 233/1000000
Iteration 234/1000000
Iteration 235/1000000
Iteration 236/1000000
Iteration 237/1000000
Iteration 238/1000000
Iteration 239/1000000
Iteration 240/1000000
Training loss: 0.004791561307998398
Training loss: 0.0030996195775555013
Training loss: 0.005703140675882904
Training loss: 0.014750941519413889
Training loss: 0.005930717973588979
Training loss: 0.003732077927227066
Training loss: 0.0030333445428494888
Training loss: 0.006168628300434537
Training loss: 0.0031663584560163594
Training loss: 0.005955173869950776
Iteration 241/1000000
Iteration 242/1000000
Iteration 243/1000000
Iteration 244/1000000
Iteration 245/1000000
Iteration 246/1000000
Iteration 247/1000000
Iteration 248/1000000
Iteration 249/1000000
Iteration 250/1000000
Training loss: 0.0648832685414255
Training loss: 0.02398870599907435
Training loss: 0.01606041850558886
Training loss: 0.00538965239990372
Training loss: 0.013851883806509015
Training loss: 0.007087255829141829
Training loss: 0.006499817076371029
Training loss: 0.013682703603781213
Training loss: 0.009662042238840811
Training loss: 0.007518284907055024
Iteration 251/1000000
Iteration 252/1000000
Iteration 253/1000000
Iteration 254/1000000
Iteration 255/1000000
Iteration 256/1000000
Iteration 257/1000000
Iteration 258/1000000
Iteration 259/1000000
Iteration 260/1000000
Training loss: 0.0561410229648871
Training loss: 0.03159309580632103
Training loss: 0.04860522742595183
Training loss: 0.06439466712043679
Training loss: 0.05373483521751859
Training loss: 0.08237676640912604
Training loss: 0.07836690164451587
Training loss: 0.04040607098052129
Training loss: 0.03612745962236177
Training loss: 0.046313389199087784
Iteration 261/1000000
Iteration 262/1000000
Iteration 263/1000000
Iteration 264/1000000
Iteration 265/1000000
Iteration 266/1000000
Iteration 267/1000000
Iteration 268/1000000
Iteration 269/1000000
Iteration 270/1000000
Training loss: 0.026881848754549246
Training loss: 0.055275558703600146
Training loss: 0.04731856404905463
Training loss: 0.029719405217475403
Training loss: 0.0502447156775696
Training loss: 0.04893256600358768
Training loss: 0.014296821343683912
Training loss: 0.05622870717837275
Training loss: 0.044209913851592064
Training loss: 0.055814890411415644
Iteration 271/1000000
Iteration 272/1000000
Iteration 273/1000000
Iteration 274/1000000
Iteration 275/1000000
Iteration 276/1000000
Iteration 277/1000000
Iteration 278/1000000
Iteration 279/1000000
Iteration 280/1000000
Training loss: 0.025472421399663242
Training loss: 0.03091431753652868
Training loss: 0.026636283501737617
Training loss: 0.019479247723591272
Training loss: 0.01659464193778539
Training loss: 0.021675440850107978
Training loss: 0.01885466344417752
Training loss: 0.017838570395054397
Training loss: 0.015726321275426672
Training loss: 0.01871494108164251
Iteration 281/1000000
Iteration 282/1000000
Iteration 283/1000000
Iteration 284/1000000
Iteration 285/1000000
Iteration 286/1000000
Iteration 287/1000000
Iteration 288/1000000
Iteration 289/1000000
Iteration 290/1000000
Training loss: 0.011600237394368452
Training loss: 0.00816721552608088
Training loss: 0.01143271434206696
Training loss: 0.016613439845270177
Training loss: 0.00612441359823645
Training loss: 0.009520804168280855
Training loss: 0.010980110833108937
Training loss: 0.010524471582479968
Training loss: 0.00766901451357257
Training loss: 0.003900557352690947
Iteration 291/1000000
Iteration 292/1000000
Iteration 293/1000000
Iteration 294/1000000
Iteration 295/1000000
Iteration 296/1000000
Iteration 297/1000000
Iteration 298/1000000
Iteration 299/1000000
Iteration 300/1000000
Training loss: 0.011792027149134379
Training loss: 0.02045090270441304
Training loss: 0.008426880935895673
Training loss: 0.006658921012674539
Training loss: 0.013407737172176786
Training loss: 0.017666482590387597
Training loss: 0.005134982953614747
Training loss: 0.00962192332817762
Training loss: 0.005783013675080828
Training loss: 0.014235083312332954
Iteration 301/1000000
Iteration 302/1000000
Iteration 303/1000000
Iteration 304/1000000
Iteration 305/1000000
Iteration 306/1000000
Iteration 307/1000000
Iteration 308/1000000
Iteration 309/1000000
Iteration 310/1000000
Training loss: 0.00531188434280521
Training loss: 0.006513748256025754
Training loss: 0.010886627629119138
Training loss: 0.009027340460011699
Training loss: 0.023094409992286173
Training loss: 0.007431730648219075
Training loss: 0.0074047616763948
Training loss: 0.01911111397801738
Training loss: 0.008772596374534512
Training loss: 0.007515213065769775
Iteration 311/1000000
Iteration 312/1000000
Iteration 313/1000000
Iteration 314/1000000
Iteration 315/1000000
Iteration 316/1000000
Iteration 317/1000000
Iteration 318/1000000
Iteration 319/1000000
Iteration 320/1000000
Training loss: 0.01590628686662822
Training loss: 0.007339359019923276
Training loss: 0.01875680738306792
Training loss: 0.021465364054495253
Training loss: 0.008484074486238979
Training loss: 0.0074696416591877165
Training loss: 0.011892440154088869
Training loss: 0.005650614822100399
Training loss: 0.00903283729915285
Training loss: 0.014592786390369058
Iteration 321/1000000
Iteration 322/1000000
Iteration 323/1000000
Iteration 324/1000000
Iteration 325/1000000
Iteration 326/1000000
Iteration 327/1000000
Iteration 328/1000000
Iteration 329/1000000
Iteration 330/1000000
Training loss: 0.0037429147165467884
Training loss: 0.004538358116511742
Training loss: 0.012568979232633087
Training loss: 0.009388691325987518
Training loss: 0.014847375923274207
Training loss: 0.005015254218588918
Training loss: 0.007223954076125088
Training loss: 0.008099078688728945
Training loss: 0.026500105626350596
Training loss: 0.020180074926489166
Iteration 331/1000000
Iteration 332/1000000
Iteration 333/1000000
Iteration 334/1000000
Iteration 335/1000000
Iteration 336/1000000
Iteration 337/1000000
Iteration 338/1000000
Iteration 339/1000000
Iteration 340/1000000
Training loss: 0.012554247667969337
Training loss: 0.015771885528379756
Training loss: 0.011484851683328797
Training loss: 0.00772885529193134
Training loss: 0.015363075005748197
Training loss: 0.008683748994318657
Training loss: 0.01919601080362026
Training loss: 0.010839476748338205
Training loss: 0.013967061233135576
Training loss: 0.005692590966009232
Iteration 341/1000000
Iteration 342/1000000
Iteration 343/1000000
Iteration 344/1000000
Iteration 345/1000000
Iteration 346/1000000
Iteration 347/1000000
Iteration 348/1000000
Iteration 349/1000000
Iteration 350/1000000
Training loss: 0.003197459613745198
Training loss: 0.010276335913014065
Training loss: 0.00433121940360078
Training loss: 0.0016787945792778717
Training loss: 0.010464197537637006
Training loss: 0.009778186036031027
Training loss: 0.01165504855507965
Training loss: 0.005726361783351497
Training loss: 0.009494991132387397
Training loss: 0.004579277238577949
Iteration 351/1000000
Iteration 352/1000000
Iteration 353/1000000
Iteration 354/1000000
Iteration 355/1000000
Iteration 356/1000000
Iteration 357/1000000
Iteration 358/1000000
Iteration 359/1000000
Iteration 360/1000000
Training loss: 0.0022522816033995656
Training loss: 0.0040585888363592355
Training loss: 0.00492798482928609
Training loss: 0.010610783084943464
Training loss: 0.011597340253964333
Training loss: 0.003174930520512297
Training loss: 0.006816205690966885
Training loss: 0.007649362953228349
Training loss: 0.004063131818489132
Training loss: 0.005928575844012601
Iteration 361/1000000
Iteration 362/1000000
Iteration 363/1000000
Iteration 364/1000000
Iteration 365/1000000
Iteration 366/1000000
Iteration 367/1000000
Iteration 368/1000000
Iteration 369/1000000
Iteration 370/1000000
Training loss: 0.005268043517780634
Training loss: 0.004983402489306487
Training loss: 0.0032622984996816
Training loss: 0.002155402169123747
Training loss: 0.0040384620378205935
Training loss: 0.004303055351628386
Training loss: 0.0019156298138263102
Training loss: 0.004608024604129067
Training loss: 0.006194548118178758
Training loss: 0.0033858460298820557
Iteration 371/1000000
Iteration 372/1000000
Iteration 373/1000000
Iteration 374/1000000
Iteration 375/1000000
Iteration 376/1000000
Iteration 377/1000000
Iteration 378/1000000
Iteration 379/1000000
Iteration 380/1000000
Training loss: 0.0016172032307532921
Training loss: 0.001996606489690606
Training loss: 0.0028914890836897623
Training loss: 0.003525869115968964
Training loss: 0.0028027856984972517
Training loss: 0.0010450975795209692
Training loss: 0.0032666805842931405
Training loss: 0.0013292991360472582
Training loss: 0.0036236262068148114
Training loss: 0.002295181393315313
Iteration 381/1000000
Iteration 382/1000000
Iteration 383/1000000
Iteration 384/1000000
Iteration 385/1000000
Iteration 386/1000000
Iteration 387/1000000
Iteration 388/1000000
Iteration 389/1000000
Iteration 390/1000000
Training loss: 0.0025084114894413497
Training loss: 0.0038440436383875413
Training loss: 0.00410910822779345
Training loss: 0.0007590558310537723
Training loss: 0.003096089091398455
Training loss: 0.0028089614484671484
Training loss: 0.005883316552758151
Training loss: 0.0023772745998343245
Training loss: 0.004010666459906509
Training loss: 0.0019291481198288314
Iteration 391/1000000
Iteration 392/1000000
Iteration 393/1000000
Iteration 394/1000000
Iteration 395/1000000
Iteration 396/1000000
Iteration 397/1000000
Iteration 398/1000000
Iteration 399/1000000
Iteration 400/1000000
Training loss: 0.0009313004168828346
Training loss: 0.002181279329111275
Training loss: 0.00141088671912028
Training loss: 0.0005046675006617309
Training loss: 0.0012173163051449937
Training loss: 0.0012355766975709133
Training loss: 0.002051621201401464
Training loss: 0.002224263107034438
Training loss: 0.0015058133449888658
Training loss: 0.0017454908877779832
Iteration 401/1000000
Iteration 402/1000000
Iteration 403/1000000
Iteration 404/1000000
Iteration 405/1000000
Iteration 406/1000000
Iteration 407/1000000
Iteration 408/1000000
Iteration 409/1000000
Iteration 410/1000000
Training loss: 0.0023500237337776525
Training loss: 0.00303635294125556
Training loss: 0.0019657354472546774
Training loss: 0.0011039394999531225
Training loss: 0.0016359363642388836
Training loss: 0.004129764911510975
Training loss: 0.001745439819690968
Training loss: 0.0053762377395481595
Training loss: 0.0011270509725069444
Training loss: 0.0019414719347164906
Iteration 411/1000000
Iteration 412/1000000
Iteration 413/1000000
Iteration 414/1000000
Iteration 415/1000000
Iteration 416/1000000
Iteration 417/1000000
Iteration 418/1000000
Iteration 419/1000000
Iteration 420/1000000
Training loss: 0.0013022431324754883
Training loss: 0.0014943959765639475
Training loss: 0.001288974747259599
Training loss: 0.0006658275885880777
Training loss: 0.00041803763842569795
Training loss: 0.00030283001585829517
Training loss: 0.0009369028754519978
Training loss: 0.0012574596480114087
Training loss: 0.001136645216370056
Training loss: 0.0008905786573914032
Iteration 421/1000000
Iteration 422/1000000
Iteration 423/1000000
Iteration 424/1000000
Iteration 425/1000000
Iteration 426/1000000
Iteration 427/1000000
Iteration 428/1000000
Iteration 429/1000000
Iteration 430/1000000
Training loss: 0.00023566067545579812
Training loss: 0.0006807489564971787
Training loss: 0.0008645559202863925
Training loss: 0.001055388705069989
Training loss: 0.0004558165423824373
Training loss: 0.0008002242553300348
Training loss: 0.0009194327508432085
Training loss: 0.0007173422067292751
Training loss: 0.0006594142390522771
Training loss: 0.0006538846501236577
Iteration 431/1000000
Iteration 432/1000000
Iteration 433/1000000
Iteration 434/1000000
Iteration 435/1000000
Iteration 436/1000000
Iteration 437/1000000
Iteration 438/1000000
Iteration 439/1000000
Iteration 440/1000000
Training loss: 0.0029871529189376515
Training loss: 0.0017010667011070678
Training loss: 0.0006884749255416812
Training loss: 0.000772093433645852
Training loss: 0.0006794322037638461
Training loss: 0.0033785981261535195
Training loss: 0.00010512805310650177
Training loss: 0.003894486858519538
Training loss: 0.0025521470928539995
Training loss: 0.001435193734106058
Iteration 441/1000000
Iteration 442/1000000
Iteration 443/1000000
Iteration 444/1000000
Iteration 445/1000000
Iteration 446/1000000
Iteration 447/1000000
Iteration 448/1000000
Iteration 449/1000000
Iteration 450/1000000
Training loss: 0.0002456143445603058
Training loss: 0.0004226869508317026
Training loss: 0.00032449134749392247
Training loss: 0.0015367672639604168
Training loss: 0.00012414421775210397
Training loss: 8.227867559860794e-05
Training loss: 0.00034943774783968105
Training loss: 0.0012064933728790656
Training loss: 0.001366294363938966
Training loss: 0.0003725755283194691
Iteration 451/1000000
Iteration 452/1000000
Iteration 453/1000000
Iteration 454/1000000
Iteration 455/1000000
Iteration 456/1000000
Iteration 457/1000000
Iteration 458/1000000
Iteration 459/1000000
Iteration 460/1000000
Training loss: 1.6366728459437593e-05
Training loss: 0.00016588743284357585
Training loss: 2.1358170070467026e-08
Training loss: 3.3898844613233152e-06
Training loss: 0.000310786809537914
Training loss: 4.3198778253568687e-07
Training loss: 0.00010076117217385778
Training loss: 0.0017711288046218785
Training loss: 3.104841388857965e-05
Training loss: 0.0004291963731634955
Iteration 461/1000000
Iteration 462/1000000
Iteration 463/1000000
Iteration 464/1000000
Iteration 465/1000000
Iteration 466/1000000
Iteration 467/1000000
Iteration 468/1000000
Iteration 469/1000000
Iteration 470/1000000
Training loss: 9.50416841800794e-10
Training loss: 3.027576194531678e-08
Training loss: 1.0985484393732744e-09
Training loss: 2.765442480276028e-08
Training loss: 2.219514418381575e-09
Training loss: 2.820770215040443e-08
Training loss: 1.359726473964094e-09
Training loss: 2.0465863494014965e-09
Training loss: 3.4456014100537486e-10
Training loss: 2.9064676490991927e-08
Iteration 471/1000000
Iteration 472/1000000
Iteration 473/1000000
Iteration 474/1000000
Iteration 475/1000000
Iteration 476/1000000
Iteration 477/1000000
Iteration 478/1000000
Iteration 479/1000000
Iteration 480/1000000
Training loss: 4.164841884413428e-10
Training loss: 4.164678085929955e-10
Training loss: 4.164149730169578e-10
Training loss: 4.1638528318637366e-10
Training loss: 4.163611804064884e-10
Training loss: 4.163594637769783e-10
Training loss: 4.163731644348692e-10
Training loss: 4.163879834835142e-10
Training loss: 4.1641010583514245e-10
Training loss: 4.1640180988441596e-10
Iteration 481/1000000
Iteration 482/1000000
Iteration 483/1000000
Iteration 484/1000000
Iteration 485/1000000
Iteration 486/1000000
Iteration 487/1000000
Iteration 488/1000000
Iteration 489/1000000
Iteration 490/1000000
Training loss: 4.163937784806467e-10
Training loss: 4.1639048892168157e-10
Training loss: 4.1638574715433154e-10
Training loss: 4.163520869499025e-10
Training loss: 4.1632177769157955e-10
Training loss: 4.1628749842880005e-10
Training loss: 4.1625373550010024e-10
Training loss: 4.1622622237001647e-10
Training loss: 4.162026669174469e-10
Training loss: 4.1618007694612455e-10
Iteration 491/1000000
Iteration 492/1000000
Iteration 493/1000000
Iteration 494/1000000
Iteration 495/1000000
Iteration 496/1000000
Iteration 497/1000000
Iteration 498/1000000
Iteration 499/1000000
Iteration 500/1000000
Training loss: 4.161620517948181e-10
Training loss: 4.161466013131937e-10
Training loss: 4.161289572232516e-10
Training loss: 4.161156733813293e-10
Training loss: 4.1610529784641695e-10
Training loss: 4.160976310724797e-10
Training loss: 4.1608771492637267e-10
Training loss: 4.1607793339941614e-10
Training loss: 4.160708558939945e-10
Training loss: 4.160655640370693e-10